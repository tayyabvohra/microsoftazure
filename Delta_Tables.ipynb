{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimAccount.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimAccount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCurrency.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCurrency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3077e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimCustomer.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimCustomer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDate.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fc8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimDepartmentGroup.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimDepartmentGroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimEmployee.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimEmployee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimGeography.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimGeography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimOrganization.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimOrganization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdba01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProduct.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProduct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductCategory.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductCategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimProductSubcategory.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimProductSubcategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimPromotion.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimPromotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V order thing you can ignore those two lines\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimReseller.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimReseller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a20896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesReason.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesReason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bc6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimSalesTerritory.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimSalesTerritory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/DimScenario.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/DimScenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCallCenter.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCallCenter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ef5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactCurrencyRate.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactCurrencyRate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb07380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactFinance.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactFinance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d210226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactInternetSales.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactInternetSales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactProductInventory.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactProductInventory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea860ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactResellerSales.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactResellerSales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSalesQuota.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSalesQuota\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to MIM for help with this code\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
    "#Load from the default lakehouse, make sure you click on the pin <=============\n",
    "from pyspark.sql.types import *\n",
    "df = spark.read.option(\"header\", \"true\").format(\"csv\").load(\"Files/AdventureWorks/FactSurveyResponse.csv\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/FactSurveyResponse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfcd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d147c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175faca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a08ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
